1、TS计算
# DataFrame中储存了十年的摘要
df = pd.DataFrame(data)

# 1. 文本相似度计算
def calculate_similarity(df):
    # 使用 TfidfVectorizer 转换文本数据为向量
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(df['summary'])
    
    # 计算相似度矩阵
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # 提取每年摘要与上一年摘要的相似度
    similarity_scores = []
    for i in range(1, len(df)):
        similarity_scores.append(similarity_matrix[i, i-1])
    
    # 将相似度添加到数据框中
    df['similarity_with_previous'] = [None] + similarity_scores  # 第一行没有相似度值
    
    return df

df = calculate_similarity(df)

# 2. 绘制相似度变化图
def plot_similarity(df):
    plt.figure(figsize=(10, 6))
    
    # 绘制文本相似度图（忽略第一年，因为没有相似度值）
    plt.plot(df['year'][1:], df['similarity_with_previous'][1:], marker='o', color='b', label='Text Similarity')

    # 图形设置
    plt.title('Text Similarity Between Annual Summaries generated by KIMI', fontsize=14)
    plt.xlabel('Year', fontsize=12)
    plt.ylabel('Cosine Similarity', fontsize=12)
    plt.xticks(df['year'])
    plt.grid(True)
    plt.legend()

    # 显示图表
    plt.show()

# 绘制相似度图
plot_similarity(df)

2、情绪利润对比
import matplotlib.pyplot as plt
from matplotlib import rcParams

# 设置中文字体，确保中文字符显示正确
rcParams['font.sans-serif'] = ['Microsoft YaHei']  # 使用微软雅黑字体，或可以使用其他支持中文的字体
rcParams['axes.unicode_minus'] = False  # 显示负号

def plot_sentiment_vs_profit(df):
    fig, ax1 = plt.subplots(figsize=(10, 5))
    
    # 绘制情绪正负比，使用第一个 y 轴
    ax1.plot(df['year'], df['sentiment_ratio'], label='Sentiment Ratio', marker='o', color='b')
    ax1.set_xlabel('Year')
    ax1.set_ylabel('Sentiment Ratio', color='b')
    ax1.tick_params(axis='y', labelcolor='b')

    # 创建第二个 y 轴，共享 x 轴
    ax2 = ax1.twinx()
    # 绘制利润总额，使用第二个 y 轴
    ax2.plot(df['year'], df['sales_revenue'], label='Sales (万元)', marker='x', color='g')
    ax2.set_ylabel('Sales (万元)', color='g')
    ax2.tick_params(axis='y', labelcolor='g')

    # 设置标题与网格
    plt.title('Sentiment Ratio vs Sales Total')
    ax1.grid(True)
    
    # 显示图例
    fig.tight_layout()  # 自动调整布局
    plt.show()

# 绘制情绪与利润对比图
plot_sentiment_vs_profit(df)


3、Bloat计算
import jieba
import matplotlib.pyplot as plt
from collections import Counter
import re

# 假设 data 字典中包含一个 "summary" 键，储存了每年对应的中文文本


# 停用词列表，可以通过网络下载中文的停用词表，或者自己定义
stop_words = set([
    '的', '了', '在', '和', '是', '我', '有', '也', '为', '与', '不', '人', '这', '就', '都', '一个', '上', '对', '我们', '你', '它', '你们', '他们'
])

# 分析每年文本的冗长性
def analyze_bloat_per_year(text):
    # 1. 使用 jieba 分词
    words = list(jieba.cut(text))
    word_count = len(words)
    
    # 2. 句子数和句子平均长度
    sentences = re.split('[。！？]', text)  # 简单通过中文标点分割句子
    sentence_count = len(sentences)
    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
    
    # 3. 停用词的比例
    filtered_words = [word for word in words if word not in stop_words]
    stopword_ratio = 1 - len(filtered_words) / word_count if word_count > 0 else 0
    
    # 4. 词汇多样性（Unique words / Total words）
    unique_words = set([word for word in words if word.isalpha()])  # 只考虑字母和汉字
    lexical_diversity = len(unique_words) / word_count if word_count > 0 else 0
    
    # 5. 中文 Flesch 可读性得分（一个简单的假设，基于句子长度和单词长度）
    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0
    flesch_score = 206.835 - 1.015 * (word_count / sentence_count) - 84.6 * (avg_word_length / word_count)  # 这是基于英文公式的简化版
    
    return {
        'word_count': word_count,
        'avg_sentence_length': avg_sentence_length,
        'stopword_ratio': stopword_ratio,
        'lexical_diversity': lexical_diversity,
        'flesch_score': flesch_score
    }

# 存储每年分析的冗长性指标
metrics = {
    'word_count': [],
    'avg_sentence_length': [],
    'stopword_ratio': [],
    'lexical_diversity': [],
    'flesch_score': [],
    'bloat_score': []
}

# 遍历数据，分析每年文本的冗长度
for i, text in enumerate(data["summary"]):
    year = 2014 + i  # 假设年份从2014开始
    year_metrics = analyze_bloat_per_year(text)
    
    # 线性组合计算冗长度评分
    bloat_score = -(
        0.2 * year_metrics['word_count'] +
        0.2 * year_metrics['avg_sentence_length'] +
        0.2 * year_metrics['stopword_ratio'] +
        0.2 * (1 - year_metrics['lexical_diversity']) + 
        0.2 * (100 - year_metrics['flesch_score'])
    )
    
    # 存储各年指标
    for metric, value in year_metrics.items():
        metrics[metric].append(value)
    
    # 存储bloat score
    metrics['bloat_score'].append(bloat_score)

# 绘制冗长度的可视化图表
years = list(range(2014, 2024))

# 创建一个新的图形，将所有子图画在一个图形里
fig, ax = plt.subplots(figsize=(10, 8))

# 设置主标题
fig.suptitle("Text Bloat Score Analysis Over 10 Years", fontsize=16)

# 画综合冗长度评分
ax.plot(years, metrics['bloat_score'], marker='o', label="Bloat Score", color='b')

# 添加标签和网格
ax.set_xlabel("Year", fontsize=12)
ax.set_ylabel("Bloat Score", fontsize=12)
ax.grid(True)

# 添加图例
ax.legend()

# 显示图形
plt.tight_layout(rect=[0, 0, 1, 0.96])  # 调整布局，给主标题留空间
plt.show()

metrics['bloat_score']

4、Pearce相关系数计算
import numpy as np
from scipy.stats import pearsonr

# 数据
kimi = [-105.88938486463972, -102.36902603973505, -130.20513102931295, -117.5737997927142, 
        -95.71919154297936, -98.49168733014335, -116.08824458866994, -90.2128246043525, 
        -145.05636708493859, -106.873217]

tongyi_qianwen = [-79.02792690532964, -181.91829992379212, -187.77528222814902, -170.09219660120735, 
                  -184.94650638048296, -167.10862666612067, -126.61604272089976, -177.85570300106463, 
                  -218.3828237854994, -175.4257585328533]

wenxin_yiyan = [-125.66497075178478, -147.09744484079803, -196.75524175219076, -144.95146636242606, 
                -186.62056699881023, -130.98075999872322, -151.37216494953833, -179.75223186070437, 
                -141.24835556790592, -222.98438456242562]

growth_rate_sales = [-20.09, 4.55, 4.06, 10.37, 39.59, 12.02, 21.33, 51.95, 0.98, 4.54]

# 检查数据长度是否一致
print(len(kimi), len(tongyi_qianwen), len(wenxin_yiyan), len(growth_rate_sales))

# 计算相关系数
kimi_corr, _ = pearsonr(kimi, growth_rate_sales)
tongyi_qianwen_corr, _ = pearsonr(tongyi_qianwen, growth_rate_sales)
wenxin_yiyan_corr, _ = pearsonr(wenxin_yiyan, growth_rate_sales)

# 输出相关系数
print(f"KIMI与年销售额增长率的相关系数: {kimi_corr:.4f}")
print(f"通义千问与年销售额增长率的相关系数: {tongyi_qianwen_corr:.4f}")
print(f"文心一言与年销售额增长率的相关系数: {wenxin_yiyan_corr:.4f}")
