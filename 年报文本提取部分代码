年报下载：
import json
import requests
import os
import re

def generate_url(stockcode, beginDate, endDate):
    """
    生成年报搜索页的url
    注意，开始日期与结束日期之间要小于三年
    stockcode： 股票代码字符串 比如'600000'
    beginDate： 开始日期字符串，如'2017-10-05'
    endDate： 结束日期字符串，如'2020-04-05'
    """
    template = 'http://query.sse.com.cn/security/stock/queryCompanyBulletin.do?jsonCallBack=jsonpCallback10470&isPagination=true&productId={stockcode}&securityType=0101&reportType2=DQBG&reportType=YEARLY&beginDate={beginDate}&endDate={endDate}&pageHelp.pageSize=25&pageHelp.pageCount=50&pageHelp.pageNo=1&pageHelp.beginPage=1&pageHelp.cacheSize=1&pageHelp.endPage=5&_=1588153366632'
    url = template.format(stockcode=stockcode, beginDate=beginDate, endDate=endDate)
    print(url)
    return url

def get_json(url):
    """
    获取url（报告列表页的url）对应的json数据
    返回raw_json
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36',
        'Referer': 'http://www.sse.com.cn/disclosure/listedinfo/regular/'}

    resp = requests.get(url, headers=headers)  # , cookies=cookies)

    raw_json = resp.text
    print(raw_json)
    return raw_json


def extract_pdf_links(raw_json):
    """
    从raw_json中抽取出pdf报告的链接link，返回的是link列表
    """
    links = []
    jsonstring = raw_json.replace('jsonpCallback10470(', '')[:-1]
    raw_dict_data = json.loads(jsonstring)
    results = raw_dict_data['result']
    for result in results:
        pattern1 = re.compile("年度报告")
        pattern2 = re.compile("摘要")
        if pattern1.findall(result['TITLE']) and not pattern2.findall(result['TITLE']):
           link = 'http://www.sse.com.cn' + result['URL']
           links.append(link)
    print(links)
    return links


def download(link, file):
    """
    下载pdf报告文件
    link： pdf报告的链接
    file: 存储文件的路径（结尾有.pdf）
    """

    resp = requests.get(link)

    # 获取到二进制数据
    binarydata = resp.content

    # 以二进制形式将数据流存入fname中
    with open(file, 'wb') as f:
        f.write(binarydata)


def main(stockcode, beginDate, endDate):
    """
    获取某stockcode在(beginDate,endDate)之间的所有pdf报告链接
    注意，开始日期与结束日期之间要小于三年
    stockcode： 股票代码字符串 比如'600000'
    beginDate： 开始日期字符串，如'2017-10-05'
    endDate： 结束日期字符串，如'2020-04-05'
    """

    print('准备采集{code}在[{begin},{end}]之间的报告'.format(code=stockcode, begin=beginDate, end=endDate))
    # 获取某公司在(beginDate,endDate)之间的所有pdf报告链接
    url = generate_url(stockcode, beginDate, endDate)
    raw_json = get_json(url)
    links = extract_pdf_links(raw_json)

    filename="年报/"+stockcode
    os.makedirs(filename,mode=0o077,exist_ok=True)

    for link in links:
        file = filename+'/{pdfname}'.format(pdfname=link.split('/')[-1])
        print('正在下载报告:{link}'.format(link=link))
        download(link, file)
    print('采集完毕 {code}在[{begin},{end}]之间的报告'.format(code=stockcode, begin=beginDate, end=endDate))

stockcode = '600171'
beginDate = '2021-6-01'
endDate = '2024-06-01'
main(stockcode, beginDate, endDate)
beginDate = '2019-6-01'
endDate = '2021-06-01'
main(stockcode, beginDate, endDate)
beginDate = '2016-6-01'
endDate = '2019-06-01'
main(stockcode, beginDate, endDate)
beginDate = '2013-6-01'
endDate = '2016-06-01'
main(stockcode, beginDate, endDate)


MD&A提取：
import numpy as np
import pandas as pd
import re
import string
import Levenshtein
import jieba
from jieba import posseg as pseg
from collections import Counter
import os

# coding:utf-8
# 从年报里面提取特定section的文本，并且清理。

#移除带有引号或括号内的停用词或短语
def remove_stop_in_quota(content):
    content = re.sub(r'[(（].{0,3}[，\,].{0,3}[\)）]', '', content)
    return content

#查找标题格式字符串
def title_like(content):
    match = re.match(r'.{0,2}[零一二三四五六七八九十\d]{1,2}.{0,2}[、)）]', content)
    if match is None:
        return False
    else:
        return True

#清理长文本中的表格内容，并根据一系列规则对文本进行过滤和处理，返回处理后的 result 列表，其中存储了经过条件判断和处理后的有效文本行。
def _clear(content):
    """
    :param content :待清理表格的长文本
    :return :清理后的文本,返回字符串
    """
    t_symbol = [u'。', u'！', u'？', u'；', u'，']
    title_t_symbol = [u'。', u'！', u'？', u'；']
    result = []
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        line = remove_stop_in_quota(line)
        # L字符串长 s空格数量 n数字数量 p终止符
        count_dist = {'l': len(line) + 1.0, 's': line.count(' '), 'n': 0, 'p': 0}
        title_p = 0
        for n in line:
            if n.isdigit():
                count_dist['n'] += 1
            if n in t_symbol:
                count_dist['p'] += 1
            if n in title_t_symbol:
                title_p += 1
        l, s, n, p = count_dist['l'], count_dist['s'], count_dist['n'], count_dist['p']
        if (n + s) / l >= 0.4 and p == 0 and count_dist['s'] != 0:  # 超过40%是空格或者字符串
            continue
        elif n + 1 == l:
            result.append('\n')
            continue
        elif s >= 2 and p == 0:  # 无终止符
            continue
        elif l <= 20 and p == 0:  # 小于20个字符
            continue
        elif title_like(line) and title_p == 0:
            continue
        result.append(line)
    return result

#这段代码的主要作用是将输入的词语和标签序列转换为一个紧凑的字符串表示形式。例如，如果 sent 是 [('Hello', 'NN'), ('world', 'NN')]，
# 则函数将返回 "HelloNNworldNN"。这种格式可能用于各种自然语言处理任务中，如词性标注或命名实体识别等，方便输出和后续处理。
def _format_result_with_tags(sent):
    result = ''
    for word, tag in sent:
        result += word + tag
    return result

#这段代码的主要作用是将输入的词语和标签序列转换为一个仅包含词语的字符串表示形式。例如，如果 sent 是 [('Hello', 'NN'), ('world', 'NN')]，
# 则函数将返回 "Hello world"。这种格式可能用于需要对文本进行简单处理或展示的场景，但不需要标签信息的情况下。
def _format_result(sent):
    result = ''
    for word, tag in sent:
        result += word
    return result


# 根据中文的句号，叹号，问号作为句子的终止符，尽可能地还原文章句子
#生成主谓结构的所有可能组合，并将这些组合存储在列表 sub_pres 中，例如，如果 subject 为 ['n', 'r']，predicate 为 ['v', 'vd']，
# 那么 sub_pres 将包含 ('n', 'v')、('n', 'vd')、('r', 'v') 和 ('r', 'vd') 这四个元组。
t_symbol = [u'。', u'！', u'？']
#  代表主语
subject = ['n', 'nr', 'ns', 'nt', 'nz', 'r', '']
#  代表谓语
predicate = ['v', 'vd', 'vn', 'z']  # 增加了'z' 状态词
#  主谓结构
sub_pres = []
for sub in subject:
    for pre in predicate:
        sub_pres.append((sub, pre))

#从输入的长文本中提取符合特定主谓结构的句子，并根据 with_tags 参数决定是否添加词性标注，最终返回处理后的句子列表。
def parse(content, with_tags=False):
    """
    :param content: 清理表格前的长文本
    :param with_tags: 返回的结果是否在体现句子主谓结构的词后添加词性标注
    :return: 句子列表
    """
    content = ''.join(_clear(content)).replace('\n', '')
    result = []
    start = 0
    format_func = _format_result_with_tags if with_tags else _format_result
    for index, item in enumerate(content):
        if item in t_symbol:
            sent = content[start:index + 1]
            start = index + 1
            token = list(pseg.cut(sent))
            for i in range(len(token) - 1):
                word, tag = token[i]
                next_word, next_tag = token[i + 1]
                if (tag, next_tag) in sub_pres:
                    sent = format_func(token)
                    result.append(sent)
                    break
    return result

#创建数字和特殊符号的匹配集合
numbers = set(string.digits + u'一二三四五六七八九十')
fuzzy_match = set(u'、（） ．.')

#函数返回一个布尔值（True 或 False），表示两个标题是否具有相同的序号前缀、后缀和类型。
def check_title_similarity(title_a, title_b):
    """检查两个标题的是否有同类型的序号，如一、二和1、2"""
    return title_a.index_prefix == title_b.index_prefix and title_a.index_suffix == title_b.index_suffix and title_a.index_type == title_b.index_type

#对给定的标题对象列表进行识别和排序，按照一级子标题和主标题的关系进行分组，并通过相似性检查来确定标题对象的归属，最终返回一个按序排列的标题序列。
def recognize(title_objects, handled_lines=None):
    """
    识别其中的标题序列,按序排好
    :param title_objects: 候选的标题
    :param handled_lines: 已经记录过的标题的行号
    :return: 标题序列
"""
    handled_lines = handled_lines or set()

    # 候选集存放候选小标题
    candidate_set = []
    # 栈存放子标题，只考虑一级子标题
    stack = []
    for i, title_object in enumerate(title_objects):
        index = title_object.index
        title = title_object.raw_title
        line_number = title_object.line_number

        # 跳过已经处理过的标题
        if line_number in handled_lines:
            continue

        if candidate_set:

            # 与栈顶标号连续，压栈
            if stack and index == stack[-1].index + 1 and check_title_similarity(title_object, stack[-1]):
                stack.append(title_object)

            # 标号为1则进盏
            elif index == 1:
                stack.append(title_object)

            # 若与候选集连续, 则加入候选集并清空盏
            elif index == candidate_set[-1].index + 1 and check_title_similarity(candidate_set[-1], title_object):
                candidate_set.append(title_object)
                handled_lines.add(title_object.line_number)
                stack.clear()

            elif stack:
                if stack[-1].index > candidate_set[-1].index and check_title_similarity(stack[-1], candidate_set[-1]):
                    tmp_l = []
                    while stack:
                        titles = stack.pop()
                        if title.index == candidate_set[-1].index:
                            while tmp_l:
                                candidate_set.append(tmp_l.pop())
                            break
                        tmp_l.append(titles)
                stack.clear()

        elif index == 1:
            candidate_set.append(title_object)
            handled_lines.add(title_object.line_number)

    if candidate_set:
        candidate_set += recognize(title_objects, handled_lines)

    return candidate_set


# coding:utf-8
chinese_numbers_trans = {
    '零': 0,
    '一': 1,
    '二': 2,
    '三': 3,
    '四': 4,
    '五': 5,
    '六': 6,
    '七': 7,
    '八': 8,
    '九': 9,
    '十': 10,
    '百': 100,
    '千': 1000,
    '万': 10000,
    '亿': 100000000
}


def convert(s):
    """将中文数转换成阿拉伯数"""
    if len(s) > 1:
        pivot = 0
        for i, letter in enumerate(s):
            if convert(letter) > convert(s[pivot]):
                pivot = i
        value = convert(s[pivot])
        lhs = convert(s[:pivot])
        rhs = convert(s[pivot + 1:])
        if lhs > 0:
            value *= lhs
        value += rhs
        return value
    elif len(s) == 0:
        return 0
    return chinese_numbers_trans[s]


# coding:utf-8
chinese_numbers = set('零一二三四五六七八九十')
digit_numbers = set(string.digits)
fuzzy_match = set(u'、（） ．.节')


class Title:
    CHINESE = 1
    DIGIT = 2

    def __init__(self, number_label, raw_title, real_title, line_number, index_prefix, index_suffix, index_type):
        self.index = number_label
        self.raw_title = raw_title
        self.real_title = real_title
        self.line_number = line_number
        self._index_prefix = None
        self._index_suffix = None
        self.index_prefix = index_prefix
        self.index_suffix = index_suffix
        self.index_type = index_type

    @property
    def index_prefix(self):
        return self._index_prefix

    @index_prefix.setter
    def index_prefix(self, value):
        self._index_prefix = Title.__standardize(value)

    @property
    def index_suffix(self):
        return self._index_suffix

    @index_suffix.setter
    def index_suffix(self, value):
        self._index_suffix = Title.__standardize(value)

    @staticmethod
    def __standardize(value):
        return value

    def __str__(self):
        return '<Title index:%s, raw_title: %s, line_number: %s>' % (self.index, self.raw_title[:10], self.line_number)


class Extractor:
    @staticmethod
    def _get_titles(content):
        titles = []
        for line_number, line in enumerate(content.splitlines()):
            if line_number == 614:
                pass
            line = line.strip()
            # 取小标题的前三个字符(unicode)与数字集做交集运算，有数字有可能是小标题
            start = line[:3]

            # 没有数字或者太短的行不是标题行
            index_type = None
            numbers = None
            start_set = set(start)
            if start_set & chinese_numbers:
                index_type = Title.CHINESE
                numbers = chinese_numbers
            elif start_set & digit_numbers:
                index_type = Title.DIGIT
                numbers = digit_numbers | {'.'}
            if not index_type or len(line) < 4:
                continue

            # 取出标号，遍历小标题的字符，取出第一个数字到不是数字为止
            index = []
            number_next = -1
            prefix = ''
            suffix = ''
            for i, char in enumerate(line):
                if char in numbers:
                    index.append(char)
                elif index and set(index) | {'.'} != set(index):
                    number_next = i
                    suffix += char
                    break
                else:
                    prefix += char
            # 数字后面2个字符以内没有分隔符说明不是小标题
            title = ''
            for i in range(number_next, min(len(line), number_next + 3)):
                if line[i] in fuzzy_match:
                    title = line[i + 1:]
                elif title:
                    break
            title = re.sub(r'[.…]+\d*$', '', title)
            if not title:
                continue

            index = ''.join(index).strip('.').split('.')[-1]
            #             print(index)
            # 汉字数转换成阿拉伯数
            if index[0] in chinese_numbers:
                index = convert(index)

            index = int(index)

            if index >= 50:
                continue
            title_object = Title(index, line, title, line_number, prefix, suffix, index_type)
            titles.append(title_object)
        return titles

    @staticmethod
    def get_title_sequence(content):
        """提取年报中的标题序列"""
        titles = Extractor._get_titles(content)
        return recognize(titles)

    @staticmethod
    def get_mda(content):
        """提取年报中管理层讨论与分析的内容"""
        content_list = content.splitlines()
        # 获取标题
        titles = Extractor.get_title_sequence(content)
        if not titles:
            return ''

        # 提取与"管理层讨论与分析"相似的标题
        min_distance = 10
        result = ''
        for i, title_obj in enumerate(titles):
            index, title, real_title = title_obj.index, title_obj.raw_title, title_obj.real_title
            real_title = ''.join(real_title.split())
            if not re.match('.*讨论与分析', title) and not re.match('.*讨论与分析', real_title):
                continue
            distance = Levenshtein.distance(real_title, '管理层讨论与分析')
            if distance < min_distance:
                if i + 1 < len(titles):
                    internal = content_list[title_obj.line_number + 1: titles[i + 1].line_number]
                else:
                    internal = content_list[title_obj.line_number + 1:]
                internal = '\n'.join(internal).strip()
                if internal:
                    min_distance = distance
                    result = internal
        if result:
            return result

        # 提取与"董事会报告"相似的标题
        min_distance = 10
        for i, title_obj in enumerate(titles):
            index, title, real_title = title_obj.index, title_obj.raw_title, title_obj.real_title
            real_title = ''.join(real_title.split())
            if not re.match('.*董事.*报告', title) and not re.match('.*董事.*报告', real_title):
                continue
            distance = Levenshtein.distance(real_title, '董事会报告')
            if distance < min_distance:
                if i + 1 < len(titles):
                    next_title = titles[i + 1]
                    internal = content_list[title_obj.line_number + 1: next_title.line_number]
                else:
                    internal = content_list[title_obj.line_number:]
                internal = '\n'.join(internal).strip()
                if internal:
                    min_distance = distance
                    result = internal
        return result

    @staticmethod
    def get_complete_sentences(content, with_tags=False):
        """提取年报中的完整句子(即主谓结构的句子)"""
        return parse(content, with_tags)


def remove_page_title(content):
    content = re.sub(r'\d*\s+\d+\s+年年度报告\s+\d+ / \d+\s+', '', content)
    content = re.sub(r"\s+上海贝岭股份有限公司\s+\d{4}\s*年年度报告\s*\d+\s+", '', content)
    return content
def remove_table(content):
    content = re.sub(r'主营业务分析[\s\S]*?关于公司未来发展的讨论与分析', '关于公司未来发展的讨论与分析', content)
    return content

def extract_content(content_):
    title_list = Extractor.get_title_sequence(content_)

    raw_title_list = [d.real_title.strip() for d in title_list]
    title_counter = Counter(raw_title_list)
    if len(title_counter) > 0:
        title_page, frequency = title_counter.most_common(1)[0]
        valid_title_page = None
        if frequency > 5:
            valid_title_page = title_page
            if not '年度' in valid_title_page:
                valid_title_page = None
    else:
        valid_title_page = None

    mda_content = Extractor.get_mda(content_)
    mda_content = remove_page_title(mda_content)
    mda_content = remove_table(mda_content)
    return mda_content


# import pdfplumber
from tika import parser
def extract_content_from_pdf(initial_file):
    try:
        rawText = parser.from_file(initial_file)
        raw_content = rawText['content']
        return raw_content
    except Exception as e:
        print(e)
        print('600171_2019_n.pdf')
        print('Failed to transform PDF file to txt file.')
        return None
    return None

for dirpath, dirnames, files in os.walk(r"C:\Users\86186\Desktop\强国课题\年报"):
    for file in files:
         if file.endswith('.pdf'):
            target_pdf=file
            file_path = os.path.abspath(os.path.join(dirpath, file))
            text_content = extract_content_from_pdf(file_path)
            mda_content = extract_content(text_content)
            outputfile = open('MD&A'+target_pdf+'.docx', 'w',encoding='utf-8')
            outputfile.write(mda_content)
            outputfile.close()
            print(target_pdf+'完成')
